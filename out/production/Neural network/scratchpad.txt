
        //NN demo
        int[] layers={3,3,1};
        Value[][] predicted;
        Network N= new Network (3, layers,"tanh");
        TrainingData td=new TrainingData();
        double[][]x=td.trainingData;
        double[][] yexpected= td.expectedoutputs;
        double[][] control_data=td.control_data;
        Normalizer Normalizer = new Normalizer();
        /*
        NN NN= new NN(3, layers,"tanh");
        NN.train(td,1e-3);
        //end of NN demo
        */
        TestMath.run();
        double [][] WW=new double [][]{
                {0.950096,   0.198722,   0.573212,   0.092996,   0.597456,   0.151454},
                {0.701818,   0.749270,   0.171409,   0.898009,   0.168973,   0.038840},
                {0.098872,   0.281751,   0.557253,   0.655506,   0.809219,   0.489511},
                {0.109544,   0.656960,   0.479024,   0.396649,   0.395940,   0.568953},
                {0.915665,   0.119756,   0.387915,   0.925864,   0.219463,   0.837738},
                {0.259116,   0.916444,   0.616096,   0.378873,   0.512405,   0.015329},
                {0.470009,   0.053936,   0.258942,   0.615391,   0.176612,   0.332770},
                {0.126189,   0.901599,   0.917955,   0.878102,   0.426310,   0.296774},
                {0.581871,   0.898102,   0.679613,   0.928316,   0.336715,   0.556313},
                {0.242777,   0.796855,   0.810921,   0.842483,   0.508972,   0.537529},
        };
        //double[][][] temppico=MathHelper.reshape(WW,1,2,-1);
        double [][] tData1=new double [][]{
                {0,0,0,1},
                {1,0,0,0},
                {0,1,0,0},
                {0,0,1,0},
        };




        int [] in1={1,3,0,2};
        int [] in2={0,1,3,2};

        double [][] Ce=new double [][]{
                {9.7459e-01,   9.8831e-01,   1.9690e-01,   2.8403e-01,   8.9925e-02},
                {3.2688e-01,   5.8263e-01,   8.7396e-01,   6.2296e-01,   8.1202e-01},
                {2.3509e-01,   3.0330e-01,   4.5127e-01,   8.6381e-01,   4.3018e-01},
                {3.2331e-01,   2.7337e-01,   7.8301e-01,   9.3656e-01,   4.5992e-01},
        };

        double [][] BB=new double [][]{
                {0.8868,   0.3591,   0.3292,   0.7945,   0.4576,   0.6388}
        };

        double [][] gamma=new double[][]{
                {.1, .5, .2, .4, .7, .3}
        };
        double [][] beta=new double[][]{
                {.2, .3, .8, .5, .9, .7}
        };

        int [] yy1s={1,3,2,1};

        double epsilon=1e-5;

        /*
        Tensor tXX1=new Tensor(4,4,new HashSet<>(),"X1").oneHot(in1);
        Tensor tXX2=new Tensor(4,4,new HashSet<>(),"X1").oneHot(in2);
        Tensor tWW1=new Tensor(WW, new HashSet<>(), "W");
        Tensor tBB1=new Tensor(BB, new HashSet<>(), "B");
        Tensor tYY1=new Tensor(4,6, new HashSet<>(), "Y1").oneHot(yy1s);
        Tensor G=new Tensor(gamma, new HashSet<>(), "gamma");
        Tensor Betha=new Tensor(beta, new HashSet<>(), "beta");
        Tensor Cemb=new Tensor(Ce, new HashSet<>(),"C");
        Tensor Emb=tXX1.dot(Cemb).join(tXX2.dot(Cemb), Tensor.Join.RIGHT);

        Tensor hpreact=Emb.dot(tWW1).addb(tBB1);
        Tensor Mean1=hpreact.mean(Tensor.Dimension.BYCOLS);

        Tensor Var1=hpreact.variance(Mean1);
        Tensor Epsilon=new Tensor (Var1.rows, Var1.cols, new HashSet<>(), "Epsilon").ones().muleach(epsilon);
        Tensor correction=Var1.add(Epsilon);
        Tensor Std1=correction.pow(0.5);
        Tensor Sub=hpreact.subb(Mean1);
        Tensor Norm1=Sub.div(Std1);




        Tensor h1=Norm1.mulb(G).addb(Betha).tanh();
        Tensor loss=h1.categoricalEntropyLoss(tYY1);


        Tensor Norm2=hpreact.batchNorm(G,Betha, Mean1.data, Var1.data, 1e-5);
        Tensor h2=Norm2.tanh();
        Tensor loss2=h2.categoricalEntropyLoss(tYY1);


        loss2.backward();
        loss2.resetGradients();
                Tensor hhh2=M.call(Emb);
        Tensor testLoss11=hhh2.categoricalEntropyLoss(tYY1);
        testLoss11.backward();

        */




        //Bigram demo


        




        Tensor W1 = null;       //Weights - act. layer
        Tensor W2 = null;       //Weights - output layer
        Tensor B1 = null;       //Bias - activation layer
        Tensor B2 = null;       //Bias - output layer
        Tensor C=null;          //embedding
        Tensor Gamma=null;      //distribution scale
        Tensor Beta=null;       // distribution shift
        Tensor Mean=null;
        Tensor Var=null;

        if (cfg.parameters.size()==cfg.getLayers()*2+3){
            W1=new Tensor(cfg.parameters.get("W1"),new HashSet<>(),"W1");
            W2=new Tensor(cfg.parameters.get("W2"),new HashSet<>(),"W2");
            B1=new Tensor(cfg.parameters.get("B1"),new HashSet<>(),"B1");
            B2=new Tensor(cfg.parameters.get("B2"),new HashSet<>(),"B2");
            C=new Tensor(cfg.parameters.get("C"),new HashSet<>(),"C");
            Gamma=new Tensor(cfg.parameters.get("Gamma"), new HashSet<>(), "Gamma");
            Beta=new Tensor(cfg.parameters.get("Beta"), new HashSet<>(), "Beta");
            Mean=new Tensor(cfg.getMeans(),new HashSet<>(),"Mean");
            Var=new Tensor(cfg.getVars(),new HashSet<>(),"Var");

        }
        else{
            W1=new Tensor(embeddingVectSize*contextSize,neuronsHid,new HashSet<>(),"W1").randTensor();
            W2=new Tensor(neuronsHid,ds.getAlphabetSize(),new HashSet<>(),"W2").randTensor();
            B1=new Tensor(1,neuronsHid,new HashSet<>(),"B1").randTensor();
            B2=new Tensor(1,ds.getAlphabetSize(),new HashSet<>(),"B2").randTensor();
            C=new Tensor(ds.getAlphabetSize(),embeddingVectSize,new HashSet<>(),"C").randTensor();
            Gamma=new Tensor(1,neuronsHid, new HashSet<>(), "Gamma").ones();
            Beta=new Tensor(1,neuronsHid, new HashSet<>(), "Beta").zeros();
            Mean=new Tensor(1,neuronsHid,new HashSet<>(),"Mean");
            Var=new Tensor(1,neuronsHid,new HashSet<>(),"Var");


        }
        parameters.add(W1);
        parameters.add(W2);
        parameters.add(B1);
        parameters.add(B2);
        parameters.add(C);
        parameters.add(Gamma);
        parameters.add(Beta);

        HashMap <String, Tensor> Params=new HashMap<>();
        Params.put("W1",W1);
        Params.put("W2",W2);
        Params.put("B1",B1);
        Params.put("B2",B2);
        Params.put("C",C);
        Params.put("Gamma",Gamma);
        Params.put("Beta",Beta);


        //fh1.SaveToJson(cfg);















        long sumTime=0;
        int epochs=100;
        Tensor D=null;
        int[][] trainingSet=ds.giveMeSet(Dataset.setType.TRAIN);
        Tensor loss1=null;

        //double [] variances=new double[epochs];
        long kAverage=0;
        double [][] means=new double [epochs][ds.getAlphabetSize()];
        double [][] vars=new double[epochs][ds.getAlphabetSize()];
        double [] losses=new double [epochs];
        double [][] moving_mean = Mean.data;
        double [][] moving_var = Var.data;
        double alpha=cfg.getAlpha();
        double [] trainLosses= new double [epochs/100];
        double [] testLosses= new double [epochs/100];
        int g=0;



        for (int epoch=0;epoch<epochs;epoch++) {

            long startTime = System.currentTimeMillis();


            double [][] batch= ds.giveMeRandomBatch(Dataset.setType.TRAIN,batchSize);
            int setLength=batch[0].length;
            Tensor X1=new Tensor(setLength,ds.getAlphabetSize(),new HashSet<>(),"X1").oneHot(batch[0]);
            Tensor X2=new Tensor(setLength,ds.getAlphabetSize(),new HashSet<>(),"X2").oneHot(batch[1]);
            Tensor X3=new Tensor(setLength,ds.getAlphabetSize(),new HashSet<>(),"X3").oneHot(batch[2]);
            Tensor Y1=new Tensor(setLength,ds.getAlphabetSize(),new HashSet<>(),"Y").oneHot(batch[3]);

            D=X1.dot(C).join(X2.dot(C), Tensor.Join.RIGHT).join(X3.dot(C), Tensor.Join.RIGHT);

            Tensor hpreactivation=D.dot(W1).addb(B1);



            //**** Batchnorm start ****//
            /*
            Tensor Mean=hpreactivation.mean(Tensor.Dimension.BYCOLS);
            Tensor Var=hpreactivation.variance(Mean);
            Tensor Std=Var.pow(0.5);
            Tensor Norm=hpreactivation.subb(Mean).div(Std);
            */
            //**** End of Batchnorm ****//

            Tensor Norm=hpreactivation.batchNorm(Gamma,Beta, Mean.data, Var.data,epsilon);
            Tensor h=Norm.tanh();
            Tensor Z=h.dot(W2).addb(B2);
            loss1=Z.categoricalEntropyLoss(Y1);


            loss1.backward();
            updateParameters(parameters,descent);
            loss1.resetGradients();

            losses[epoch]=loss1.data[0][0];



            long endTime = System.currentTimeMillis();
            long millis = endTime-startTime;

            sumTime+=millis;
            kAverage+=millis;
            Mean=hpreactivation.mean(Tensor.Dimension.BYCOLS);
            Var=hpreactivation.variance(Mean);

            for (int i=0; i<neuronsHid;i++){
                moving_mean[0][i]=alpha*moving_mean[0][i]+(1-alpha)*Mean.data[0][i];
                moving_var[0][i]=alpha*moving_var[0][i]+(1-alpha)*Var.data[0][i];
            }

            if (epoch%100==0) {
                System.out.println("Epoch " + (int) (epoch + cfg.getEpochsLasted()) + " Loss: " + loss1.data[0][0] + " | last batch time: " + millis + " ms | "+epoch+"/"+epochs+" average: "+kAverage+" ms");
                trainLosses[g]=loss1.data[0][0];
                //testLosses[g]=splitLoss(ds, Dataset.setType.TEST,cfg.batchSize,Params,moving_mean,moving_var);
                g++;
                kAverage=0;
            }


            if (epoch>epochs/3) descent=0.05;
            if (epoch>epochs*(3/4)) descent=0.01;




        }


        //Chart trainChart = new Chart(trainLosses);
        Chart chart = new Chart("Losses Over Time", "Epoch", "Loss");
        chart.addSeries(testLosses);
        chart.addSeries(trainLosses);
        chart.display();



        //System.out.println("Average epoch time: "+sumTime/epochs+"ms, Loss: "+loss1.data[0][0]);

        cfg.storeParameters(parameters);
        cfg.setEpochsLasted(epochs);
        cfg.setMeans(moving_mean);
        cfg.setVars(moving_var);
        fh1.SaveToJson(cfg);

        double testLoss=splitLoss(ds, Dataset.setType.TEST,cfg.batchSize,Params,moving_mean,moving_var);
        double trainLoss=splitLoss(ds, Dataset.setType.TRAIN,cfg.batchSize,Params,moving_mean,moving_var);

        System.out.println(" Train set Loss: "+trainLoss);
        System.out.println(" Test set Loss: "+testLoss);





        int t=0;
        int f=0;
        //generate strings based on calculated weights
        Mean=new Tensor (moving_mean,new HashSet<>(),"moving mean");
        Var=new Tensor (moving_var,new HashSet<>(),"moving variance");
        Tensor Std=Var.pow(0.5);

        Random random=new Random();
        for (int i=0;i<50;i++){


            String output="";
            String context="";
            for (int j = 0; j < cfg.getContextSize(); j++) {
                context += ".";
            }

            while (true){
                int [][] cArray=new int [cfg.getContextSize()][1];
                for (int k=0;k<context.length();k++){
                        cArray[k][0]= ds.strtoi(context.charAt(k));
                }

                Tensor X1=new Tensor(1,ds.getAlphabetSize(),new HashSet<>(),"X1").oneHot(cArray[0]);
                Tensor X2=new Tensor(1,ds.getAlphabetSize(),new HashSet<>(),"X2").oneHot(cArray[1]);
                Tensor X3=new Tensor(1,ds.getAlphabetSize(),new HashSet<>(),"X3").oneHot(cArray[2]);
                D=X1.dot(C).join(X2.dot(C), Tensor.Join.RIGHT).join(X3.dot(C), Tensor.Join.RIGHT);
                //S batchnorm
                Tensor hpreactivation=D.dot(W1).addb(B1);
                Tensor Norm=hpreactivation.subb(Mean).div(Std);
                Tensor h=Norm.mulb(Gamma).addb(Beta).tanh();
                Tensor Z=h.dot(W2).addb(B2);
                Tensor P=Z.softMax();


                int[] vector= MathHelper.generateMultinomialVector(1, P.data[0], random);
                output=output+ds.itostr(vector);
                if (vector[0]==0){
                    break;
                }
                String tmp="";
                for (int k=0;k<cfg.getContextSize()-1;k++){
                    tmp+=context.charAt((k+1));
                }
                context=tmp;
                context+=ds.itostr(vector);

            }
            System.out.println(output);

        }
        fh1.SaveToJson(cfg);



        //End of Bigram demo



    }

    public static void updateParameters(HashSet<Tensor> param, double descent){
        for (Tensor p:param) {
            //if (p.label!="C") {
                for (int i = 0; i < p.data.length; i++) {
                    for (int j = 0; j < p.data[0].length; j++) {
                        p.data[i][j] += -descent * p.gradients[i][j];
                    }
                }
            //}
        }
    }


    public static double batchLoss(double[][] set, int alphabetSize,HashMap<String, Tensor> parameters, double[][] moving_mean, double[][] moving_var){
        int setLength=set[0].length;
        Tensor X1=new Tensor(setLength,alphabetSize,new HashSet<>(),"X1").oneHot(set[0]);
        Tensor X2=new Tensor(setLength,alphabetSize,new HashSet<>(),"X2").oneHot(set[1]);
        Tensor X3=new Tensor(setLength,alphabetSize,new HashSet<>(),"X3").oneHot(set[2]);
        Tensor Y1=new Tensor(setLength,alphabetSize,new HashSet<>(),"Y").oneHot(set[3]);
        Tensor C=parameters.get("C");
        Tensor W1=parameters.get("W1");
        Tensor W2=parameters.get("W2");
        Tensor B1=parameters.get("B1");
        Tensor B2=parameters.get("B2");
        Tensor Gamma=parameters.get("Gamma");
        Tensor Beta=parameters.get("Beta");
        Tensor D=X1.dot(C).join(X2.dot(C), Tensor.Join.RIGHT).join(X3.dot(C), Tensor.Join.RIGHT);
        //S batchnorm
        Tensor hpreactivation=D.dot(W1).addb(B1);
        Tensor Mean=null;
        Tensor Var=null;

        Mean=new Tensor(moving_mean, new HashSet<>(), "Mean");
        Var=new Tensor(moving_var, new HashSet<>(), "Var");

        Tensor Std=Var.pow(0.5);
        Tensor Norm=hpreactivation.subb(Mean).div(Std);
        Tensor h=Norm.mulb(Gamma).addb(Beta).tanh();
        Tensor Z=h.dot(W2).addb(B2);
        //Batchnorm
        //Tensor Z=D.dot(W1).addb(B1).tanh().dot(W2).addb(B2);
        Tensor loss1=Z.categoricalEntropyLoss(Y1);
        return loss1.data[0][0];
    }

    public static double splitLoss(Dataset ds, Dataset.setType type, int batchSize,HashMap<String, Tensor> parameters, double[][] moving_mean, double[][] moving_var){
        double loss=0;
        int setSize=ds.giveMeSet(type)[0].length;
        int nBatches=(int)(Math.floor((double)setSize/batchSize));
        //TODO: needs to be fixed. Last batch, which is smaller then batchSize is ignored.
        for (int i=0;i<nBatches;i++){
            int startIndex=i*batchSize;
            if (i*batchSize+batchSize>setSize){
                batchSize=(i*nBatches+batchSize)-setSize;
            }
            double [][] batch=ds.giveMeBatch(batchSize,startIndex,type);
            loss+=batchLoss(batch,ds.getAlphabetSize(),parameters, moving_mean, moving_var);
        }
        return loss/nBatches;
    }